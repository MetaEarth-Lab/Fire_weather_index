{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from netCDF4 import Dataset, date2num\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from shapely import vectorized\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ê° ê¸°í›„ ëª¨ë¸ë³„ mean, std, max ê°’ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GCMs_list = ['GFDL-ESM4', 'IPSL-CM6A-LR', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'UKESM1-0-LL']\n",
    "data_types = [\"historical\", \"picontrol\"]\n",
    "\n",
    "shapefile_path = \"./shape_file_10m_cultural/ne_10m_admin_0_countries.shp\"\n",
    "gdf_borders = gpd.read_file(shapefile_path)\n",
    "korea = gdf_borders[gdf_borders['SOVEREIGNT'].isin(['South Korea', 'North Korea'])].geometry.unary_union\n",
    "\n",
    "lon = np.load(\"../data/ISIMIP_ko_lon_2km.npy\")  # (601,)\n",
    "lat = np.load(\"../data/ISIMIP_ko_lat_2km.npy\")  # (601,)\n",
    "\n",
    "lon_grid, lat_grid = np.meshgrid(lon, lat)\n",
    "mask = vectorized.contains(korea, lon_grid, lat_grid)\n",
    "\n",
    "for data_type_ in data_types:\n",
    "    for GCM in GCMs_list:\n",
    "        # ğŸ“Œ ë°ì´í„°ê°€ ì €ì¥ëœ ê²½ë¡œ\n",
    "        data_dir = f\"../result/HR/{data_type_}/{GCM}/origin_fwi\"\n",
    "        result_path = f\"../result/{data_type_}/{GCM}\"\n",
    "        # ğŸ“Œ ë‚ ì§œë³„ ë°ì´í„°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "        daily_data = defaultdict(list)\n",
    "\n",
    "        # ğŸ“Œ ëª¨ë“  íŒŒì¼ì„ ê°€ì ¸ì™€ ì›”/ì¼ë³„ ê·¸ë£¹í™”\n",
    "        for file_name in sorted(os.listdir(data_dir)):\n",
    "            if file_name.endswith(\".npy\") and file_name.startswith(\"20\"):  # 2000~2014ë…„ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "                mmdd = file_name[4:8]  # 'MMDD' ì¶”ì¶œ (ex: 0101, 0215, ...)\n",
    "                if mmdd == \"0229\":\n",
    "                    continue\n",
    "                file_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "                data = np.load(file_path)  # (601, 601) í˜•íƒœ\n",
    "\n",
    "                data = np.where(np.flipud(mask), data, np.nan)\n",
    "                daily_data[mmdd].append(data)\n",
    "\n",
    "        # ğŸ“Œ ê²°ê³¼ë¥¼ ì €ì¥í•  ë°°ì—´ (365ì¼ ë˜ëŠ” 366ì¼)\n",
    "        num_days = len(daily_data)  # 365 ë˜ëŠ” 366\n",
    "        sample_shape = list(daily_data.values())[0][0].shape  # (601, 601)\n",
    "        daily_mean = np.zeros((num_days, *sample_shape), dtype=np.float32)\n",
    "        daily_std = np.zeros((num_days, *sample_shape), dtype=np.float32)\n",
    "        daily_max = np.zeros((num_days, *sample_shape), dtype=np.float32)\n",
    "\n",
    "        # ğŸ“Œ ê° ì›”/ì¼ë³„ í‰ê·  & í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "        for i, (mmdd, data_list) in enumerate(sorted(daily_data.items())):\n",
    "            data_stack = np.stack(data_list)  # (15, 601, 601)\n",
    "            daily_mean[i] = np.mean(data_stack, axis=0)  # í‰ê·  ê³„ì‚°\n",
    "            daily_std[i] = np.std(data_stack, axis=0)  # í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "            daily_max[i] = np.max(data_stack, axis=0)  # max ê³„ì‚°\n",
    "            print(f\"âœ… {mmdd} ì™„ë£Œ: {len(data_list)}ë…„ ë°ì´í„° ì²˜ë¦¬ë¨\")\n",
    "\n",
    "        # ğŸ“Œ ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "        np.save(os.path.join(result_path, \"daily_mean_ko.npy\"), daily_mean)\n",
    "        np.save(os.path.join(result_path, \"daily_std_ko.npy\"), daily_std)\n",
    "        np.save(os.path.join(result_path, \"daily_max_ko.npy\"), daily_max)\n",
    "\n",
    "print(\"ğŸ¯ ëª¨ë“  ì›”/ì¼ë³„ mean, std, max ê³„ì‚° ì™„ë£Œ ë° ì €ì¥ë¨!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ê¸°í›„ ëª¨ë¸ ì·¨í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GCM ë¦¬ìŠ¤íŠ¸\n",
    "GCMs_list = ['GFDL-ESM4', 'IPSL-CM6A-LR', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'UKESM1-0-LL']\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ëˆ„ì í•  ë¦¬ìŠ¤íŠ¸\n",
    "data_list = []\n",
    "\n",
    "# ê° GCMë³„ë¡œ íŒŒì¼ ë¡œë“œ (íŒŒì¼ ê²½ë¡œëŠ” ì˜ˆì‹œë¡œ êµ¬ì„±ë¨)\n",
    "for gcm in GCMs_list:\n",
    "    file_path = f\"/lustre/home/ebcho/workspace/pyfwi/SR_ISIMIP/result/historical/{gcm}/daily_max_ko.npy\"\n",
    "    try:\n",
    "        arr = np.load(file_path)  # (365, 601, 601)\n",
    "        data_list.append(arr)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {gcm} íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "# í‰ê·  ê³„ì‚°\n",
    "if data_list:\n",
    "    stacked = np.stack(data_list, axis=0)  # shape: (5, 365, 601, 601)\n",
    "    mean_result = np.mean(stacked, axis=0)  # shape: (365, 601, 601)\n",
    "else:\n",
    "    mean_result = None\n",
    "    print(\"ğŸ“‚ ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ\")\n",
    "\n",
    "np.save(\"/lustre/home/ebcho/workspace/pyfwi/SR_ISIMIP/result/historical/all_gcm_fwi.npy\", mean_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. nc íŒŒì¼ë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ”¹ ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "fwi_path = \"/lustre/home/ebcho/workspace/pyfwi/SR_ISIMIP/result/historical/all_gcm_fwi.npy\"\n",
    "output_nc_path = \"/lustre/home/ebcho/workspace/pyfwi/SR_ISIMIP/result/historical/all_gcm_fwi.nc\"\n",
    "\n",
    "\n",
    "# ğŸ”¹ ì²« ë²ˆì§¸ íŒŒì¼ ë¡œë“œí•˜ì—¬ shape í™•ì¸\n",
    "all_fwi = np.load(fwi_path)\n",
    "lat_dim, lon_dim = all_fwi[0].shape[0], all_fwi[0].shape[1]\n",
    "\n",
    "\n",
    "# ğŸ”¹ ìœ„ë„, ê²½ë„ ë°ì´í„° ë¡œë“œ\n",
    "lat = np.load(\"./data/ISIMIP_ko_lat_2km.npy\")\n",
    "lon = np.load(\"./data/ISIMIP_ko_lon_2km.npy\")\n",
    "\n",
    "mean_std_times = [datetime(2000, 1, 1) + timedelta(days=i) for i in range(365)]\n",
    "day_of_year = np.arange(1, 366)  # 1~365ì¼\n",
    "# print(len(mean_std_times), len(day_of_year))\n",
    "\n",
    "# ğŸ“Œ NetCDF íŒŒì¼ ìƒì„± (`xarray.Dataset`)\n",
    "ds = xr.Dataset(\n",
    "    {\n",
    "        \"FWI_all\": ([\"time_mean\", \"lat\", \"lon\"], all_fwi),  # FWI (2000~2014)\n",
    "        \"day_of_year\": ([\"time_mean\"], day_of_year),  # ì¶”ê°€ëœ day_of_year ë³€ìˆ˜\n",
    "    },\n",
    "    coords={\n",
    "        \"time_mean\": mean_std_times,  # 1ë…„ 365ì¼ (2000ë…„ ê¸°ì¤€)\n",
    "        \"lat\": lat,\n",
    "        \"lon\": lon,\n",
    "    }\n",
    ")\n",
    "\n",
    "# ğŸ“Œ NetCDF íŒŒì¼ ì €ì¥\n",
    "ds.to_netcdf(output_nc_path)\n",
    "print(f\"âœ… NetCDF íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_nc_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
